{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install noisereduce\n",
    "!pip install python_speech_features\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install SpeechRecognition\n",
    "!pip install python_speech_features\n",
    "!pip install --upgrade librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc , logfbank\n",
    "import librosa as lr\n",
    "import os, glob, pickle\n",
    "import librosa\n",
    "from scipy import signal\n",
    "import noisereduce as nr\n",
    "from glob import glob\n",
    "import librosa\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import soundfile\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D, Flatten, LSTM\n",
    "from keras.layers import Dropout,Dense,TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy.fftpack import fft\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirName = 'C:/Users/Admin/Desktop/RAW'\n",
    "dir_path = 'C:/Users/Admin/Desktop/RAW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(dir_path):\n",
    "    for file in filenames:\n",
    "        if file.endswith('.wav'):\n",
    "            list_of_files.append(os.path.join(dirpath, file))\n",
    "\n",
    "num_audio_files = len(list_of_files)\n",
    "print(f'Total number of audio files: {num_audio_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(dirName):\n",
    "    listOfFile=os.listdir(dirName)\n",
    "    allFiles=list()\n",
    "    for entry in listOfFile:\n",
    "        fullPath=os.path.join(dirName, entry)\n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles=allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "    return allFiles\n",
    "\n",
    "dirName = 'C:/Users/Admin/Desktop/RAW'\n",
    "listOfFiles = getListOfFiles(dirName)\n",
    "len(listOfFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "r=sr.Recognizer()\n",
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "    with sr.AudioFile(listOfFiles[file]) as source:\n",
    "        audio = r.listen(source)\n",
    "        try:\n",
    "            text = r.recognize_google(audio)\n",
    "            print(text)\n",
    "        except:\n",
    "            print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def envelope(y , rate, threshold):\n",
    "    mask=[]\n",
    "    y=pd.Series(y).apply(np.abs)\n",
    "    y_mean = y.rolling(window=int(rate/10) ,  min_periods=1 , center = True).mean()\n",
    "    for mean in y_mean:\n",
    "        if mean>threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "    audio , sfreq = lr.load(listOfFiles[file])\n",
    "    time = np.arange(0 , len(audio)) / sfreq\n",
    "    \n",
    "    fig ,ax = plt.subplots()\n",
    "    ax.plot(time , audio)\n",
    "    ax.set(xlabel = 'Time (s)' , ylabel = 'Sound Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "     sample_rate , samples = wavfile.read(listOfFiles[file])\n",
    "     frequencies , times, spectrogram = signal.spectrogram(samples, sample_rate) \n",
    "     plt.pcolormesh(times, frequencies, spectrogram)\n",
    "     plt.imshow(spectrogram)\n",
    "     plt.ylabel('Frequency [Hz]')\n",
    "     plt.xlabel('Time [sec]')\n",
    "     plt.show()\n",
    "     #14min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.dtype)\n",
    "print(spectrogram.shape)\n",
    "print(spectrogram.dtype)\n",
    "#spectrogram has 129 frequency bins and 779 time slices#\n",
    "plt.pcolormesh(times, frequencies, spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in range(len(listOfFiles)):\n",
    "#    sample_rate, samples = wavfile.read(listOfFiles[file])\n",
    "#    frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "#\n",
    "#    plt.pcolormesh(times, frequencies, spectrogram)\n",
    "#    plt.ylabel('Frequency [Hz]')\n",
    "#    plt.xlabel('Time [sec]')\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = np.transpose(spectrogram)\n",
    "times = times.reshape(-1, 1)\n",
    "frequencies = frequencies.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in range(len(listOfFiles)):\n",
    "    sample_rate, samples = wavfile.read(listOfFiles[file])\n",
    "    frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "\n",
    "    plt.pcolormesh(times, frequencies, spectrogram)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()\n",
    "#6min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signals(signals):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Time Series' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(signals.keys())[i])\n",
    "            axes[x,y].plot(list(signals.values())[i])\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "\n",
    "def plot_fft(fft):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Fourier Transform' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            data = list(fft.values())[i]\n",
    "            Y,freq = data[0] , data[1]\n",
    "            axes[x,y].set_title(list(fft.keys())[i])\n",
    "            axes[x,y].plot(freq , Y)\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "    \n",
    "def plot_fbank(fbank):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Filter Bank Coefficients' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(fbank.keys())[i])\n",
    "            axes[x,y].imshow(list(fbank.values())[i],cmap='hot', interpolation = 'nearest')\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "            \n",
    "def plot_mfccs(mfccs):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Mel Frequency Capstrum  Coefficients' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(mfccs.keys())[i])\n",
    "            axes[x,y].imshow(list(mfccs.values())[i],\n",
    "                             cmap='hot', interpolation = 'nearest')\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "\n",
    "def calc_fft(y,rate):\n",
    "    n = len(y)\n",
    "    freq = np.fft.rfftfreq(n , d= 1/rate)\n",
    "    Y= abs(np.fft.rfft(y)/n)\n",
    "    return(Y,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(glob.glob('C:\\\\Users\\\\Admin\\\\Desktop\\\\RAW\\\\**\\**\\*.wav', recursive=True)):\n",
    "    file_name = os.path.basename(file)\n",
    "    signal, rate = librosa.load(file, sr=16000)\n",
    "    mask = envelope(signal, rate, 0.0005)\n",
    "    wavfile.write(filename='C:\\\\Users\\\\Admin\\\\Desktop\\\\CLEAN\\\\' + str(file_name), rate=rate, data=signal[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = None\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            mfccs = mfccs.reshape(1, -1)  # Reshape to have a single row\n",
    "            result = mfccs\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "            if result is None:\n",
    "                result = chroma.reshape(1, -1)\n",
    "            else:\n",
    "                chroma = chroma.reshape(1, -1)\n",
    "                result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "            if result is None:\n",
    "                result = mel.reshape(1, -1)\n",
    "            else:\n",
    "                mel = mel.reshape(1, -1)\n",
    "                result = np.hstack((result, mel))\n",
    "    return result\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\Admin\\\\Desktop\\\\CLEAN\\\\03-01-01-01-01-01-02.wav\"\n",
    "mfcc = True\n",
    "chroma = True\n",
    "mel = True\n",
    "\n",
    "features = extract_feature(file_path, mfcc, chroma, mel)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "observed_emotions=['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\Admin\\\\Desktop\\\\CLEAN\\\\03-01-01-01-01-01-02.wav\"\n",
    "mfcc = True\n",
    "chroma = True\n",
    "mel = True\n",
    "features = extract_feature(file_path, mfcc, chroma, mel)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, test_size=0.33):\n",
    "    x, y = [], []\n",
    "    for file in glob.glob(os.path.join(path, '*.wav')):\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append([emotion, file_name])\n",
    "        print(\"Number of samples:\", len(x))\n",
    "        print(\"Number of labels:\", len(y))\n",
    "        return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size=0.33):\n",
    "    x,y=[],[]\n",
    "    answer = 0\n",
    "    for file in glob.glob(r'C:\\\\Users\\\\Admin\\\\Desktop\\\\CLEAN\\\\*.wav'):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            answer += 1\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append([emotion,file_name])\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_data(test_size=0.25)\n",
    "print(np.shape(x_train), np.shape(x_test), np.shape(y_train), np.shape(y_test))\n",
    "\n",
    "print('Sample feature:', x_train[0])\n",
    "print('Test ','Sample label:', y_train[0])\n",
    "\n",
    "y_test_map = np.array(y_test).T\n",
    "y_test = y_test_map[0]\n",
    "test_filename = y_test_map[1]\n",
    "y_train_map = np.array(y_train).T\n",
    "y_train = y_train_map[0]\n",
    "train_filename = y_train_map[1]\n",
    "print(np.shape(y_train), np.shape(y_test))\n",
    "print(*test_filename, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((x_train[0], x_test[0]))\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "y_train = y_train.reshape(y_train.shape[0],)\n",
    "\n",
    "model = MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkl_Filename = \"Emotion_Voice_Detection_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model back from file\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Emotion_Voice_Detection_Model = pickle.load(file)\n",
    "\n",
    "Emotion_Voice_Detection_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_test\n",
    "num_samples, num_features, num_time_steps = x_test.shape\n",
    "x_test_reshaped = x_test.reshape(num_samples, num_features * num_time_steps)\n",
    "\n",
    "# Make predictions on the reshaped data\n",
    "y_pred = model.predict(x_test_reshaped)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the Prediction probabilities into CSV file \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "y_pred1 = pd.DataFrame(y_pred, columns=['predictions'])\n",
    "y_pred1['file_names'] = test_filename\n",
    "print(y_pred1)\n",
    "y_pred1.to_csv('predictionfinal.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record audio using audio_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load audio file\n",
    "file_path = 'output10.wav'\n",
    "data, sampling_rate = librosa.load(file_path)\n",
    "\n",
    "# Extract features\n",
    "mfccs = librosa.feature.mfcc(y=data, sr=sampling_rate)\n",
    "chroma = librosa.feature.chroma_stft(y=data, sr=sampling_rate)\n",
    "mel = librosa.feature.melspectrogram(y=data, sr=sampling_rate)\n",
    "\n",
    "# Plot features\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# MFCC\n",
    "plt.subplot(3, 1, 1)\n",
    "librosa.display.specshow(mfccs, x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('MFCC')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('MFCC Coefficients')\n",
    "\n",
    "# Chroma\n",
    "plt.subplot(3, 1, 2)\n",
    "librosa.display.specshow(chroma, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Chroma')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pitch Class')\n",
    "\n",
    "# Mel spectrogram\n",
    "plt.subplot(3, 1, 3)\n",
    "librosa.display.specshow(librosa.power_to_db(mel, ref=np.max), x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Plot waveform\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(data)\n",
    "plt.title('Waveform')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import numpy as np\n",
    "\n",
    "file = \"C:\\\\Users\\\\Admin\\\\Desktop\\\\TEST Audio\\\\Test 2.wav\"\n",
    "\n",
    "ans = []\n",
    "new_feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "ans.append(new_feature)\n",
    "ans = np.array(ans)\n",
    "\n",
    "# Reshape the ans array to have 2 dimensions\n",
    "ans = np.reshape(ans, (ans.shape[0], -1))\n",
    "\n",
    "prediction = Emotion_Voice_Detection_Model.predict(ans)\n",
    "print(prediction)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REDIRECT THE OUTPUT TO music_recommendation "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
